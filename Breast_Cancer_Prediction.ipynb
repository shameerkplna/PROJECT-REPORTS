{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dkOgXEqF94DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUin6cuBbjv6",
        "outputId": "3fd18d4a-11ed-4551-eee7-3f7d08ab6936"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values per column:\n",
            " mean radius                0\n",
            "mean texture               0\n",
            "mean perimeter             0\n",
            "mean area                  0\n",
            "mean smoothness            0\n",
            "mean compactness           0\n",
            "mean concavity             0\n",
            "mean concave points        0\n",
            "mean symmetry              0\n",
            "mean fractal dimension     0\n",
            "radius error               0\n",
            "texture error              0\n",
            "perimeter error            0\n",
            "area error                 0\n",
            "smoothness error           0\n",
            "compactness error          0\n",
            "concavity error            0\n",
            "concave points error       0\n",
            "symmetry error             0\n",
            "fractal dimension error    0\n",
            "worst radius               0\n",
            "worst texture              0\n",
            "worst perimeter            0\n",
            "worst area                 0\n",
            "worst smoothness           0\n",
            "worst compactness          0\n",
            "worst concavity            0\n",
            "worst concave points       0\n",
            "worst symmetry             0\n",
            "worst fractal dimension    0\n",
            "target                     0\n",
            "dtype: int64\n",
            "Confusion Matrix:\n",
            " [[41  2]\n",
            " [ 1 70]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.95      0.96        43\n",
            "         1.0       0.97      0.99      0.98        71\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.97      0.97      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Preprocess the data to handle any missing values and perform necessary feature scaling.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Load the dataset\n",
        "data = load_breast_cancer()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# 1. Handle Missing Values (if any)\n",
        "print(\"Missing values per column:\\n\", df.isnull().sum())\n",
        "\n",
        "# Impute missing values (if any exist) using mean for numerical features\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
        "\n",
        "# 2. Feature Scaling (Standardization)\n",
        "scaler = StandardScaler()\n",
        "X = df_imputed.drop('target', axis=1) # Features\n",
        "y = df_imputed['target'] # Target variable\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 3. Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 4. Model Training (example: Logistic Regression)\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 5. Model Evaluation\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Implement the following five classification algorithms:\n",
        "# 1. Logistic Regression\n",
        "# 2. Decision Tree Classifier\n",
        "# 3. Random Forest Classifier\n",
        "# 4. Support Vector Machine (SVM)\n",
        "# 5. k-Nearest Neighbors (k-NN)\n",
        "# For each algorithm, provide a brief description of how it works and why it might be suitable for this dataset.\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "classifiers = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=10000),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"SVM\": SVC(),\n",
        "    \"k-NN\": KNeighborsClassifier()\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for name, clf in classifiers.items():\n",
        "    clf.fit(X_train_scaled, y_train)\n",
        "    y_pred = clf.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    results[name] = accuracy\n",
        "    print(f\"{name} accuracy: {accuracy}\")\n",
        "\n",
        "print(\"\\nAlgorithm Descriptions and Suitability:\")\n",
        "print(\"Logistic Regression: Models the probability of a binary outcome. Suitable for linearly separable data or when a probability estimate is needed.\")\n",
        "print(\"Decision Tree: Creates a tree-like model of decisions and their possible consequences. Suitable for both linear and non-linear data and provides interpretability.\")\n",
        "print(\"Random Forest: An ensemble method that combines multiple decision trees to improve accuracy and reduce overfitting.  Suitable for complex datasets, often performs well.\")\n",
        "print(\"SVM: Finds an optimal hyperplane to separate data points into different classes. Suitable for both linear and non-linear data (using kernels), effective in high-dimensional spaces.\")\n",
        "print(\"k-NN: Classifies data points based on the majority class among its k nearest neighbors. Suitable for non-linear data, but can be computationally expensive for large datasets.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aULb8oH2qci5",
        "outputId": "16a5b08a-0b0b-4671-f2dd-720650eeb1dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression accuracy: 0.9736842105263158\n",
            "Decision Tree accuracy: 0.9385964912280702\n",
            "Random Forest accuracy: 0.9649122807017544\n",
            "SVM accuracy: 0.9824561403508771\n",
            "k-NN accuracy: 0.9473684210526315\n",
            "\n",
            "Algorithm Descriptions and Suitability:\n",
            "Logistic Regression: Models the probability of a binary outcome. Suitable for linearly separable data or when a probability estimate is needed.\n",
            "Decision Tree: Creates a tree-like model of decisions and their possible consequences. Suitable for both linear and non-linear data and provides interpretability.\n",
            "Random Forest: An ensemble method that combines multiple decision trees to improve accuracy and reduce overfitting.  Suitable for complex datasets, often performs well.\n",
            "SVM: Finds an optimal hyperplane to separate data points into different classes. Suitable for both linear and non-linear data (using kernels), effective in high-dimensional spaces.\n",
            "k-NN: Classifies data points based on the majority class among its k nearest neighbors. Suitable for non-linear data, but can be computationally expensive for large datasets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the performance of the five classification algorithms.\n",
        "# Which algorithm performed the best and which one performed the worst?\n",
        "\n",
        "best_algorithm = max(results, key=results.get)\n",
        "worst_algorithm = min(results, key=results.get)\n",
        "\n",
        "print(f\"\\nBest performing algorithm: {best_algorithm} with accuracy {results[best_algorithm]}\")\n",
        "print(f\"Worst performing algorithm: {worst_algorithm} with accuracy {results[worst_algorithm]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htqGaXgO27lK",
        "outputId": "0d708e79-b36f-4f39-f9a5-213377069a6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best performing algorithm: SVM with accuracy 0.9824561403508771\n",
            "Worst performing algorithm: Decision Tree with accuracy 0.9385964912280702\n"
          ]
        }
      ]
    }
  ]
}